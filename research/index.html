<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Publications | Fei Qi</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Generated by jekyll-scholar." />
<meta property="og:description" content="Generated by jekyll-scholar." />
<link rel="canonical" href="/research/" />
<meta property="og:url" content="/research/" />
<meta property="og:site_name" content="Fei Qi" />
<script type="application/ld+json">
{"headline":"Publications","@type":"WebPage","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"url":"/research/","description":"Generated by jekyll-scholar.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Fei Qi" />
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="canonical" href="/research/">
<!-- Bootstrap CSS 4.4.1-->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<!-- KaTeX 0.12.0-->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />
<!-- Font Awesome CSS 5.13.1-->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.1/css/all.min.css" integrity="sha512-xA6Hp6oezhjd6LiLZynuukm80f8BoZ3OpcEYaqKoCV3HKQDrYjDE1Gu8ocxgxoXmwmSzM4iqPvCsOkQNiu41GA==" crossorigin="anonymous">
<!-- Academicons 1.9.0-->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<!-- Google Fonts CSS -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

<link rel="stylesheet" href="/assets/css/main.css" />
<!-- jQuery JS 3.4.1 -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
	integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<!-- popper JS 1.16.0 -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
	integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<!-- Bootstrap JS 4.4.1 -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
	integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<!-- KaTeX 0.12.0 -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
	integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" defer crossorigin="anonymous"></script>
<!-- KaTeX render 0.12.0 -->
<script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
	integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" defer onload="renderMathInElement(document.body);"  crossorigin="anonymous"></script>

<script src="/assets/js/common.js"></script>

  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      <a class="navbar-brand title font-weight-lighter" href="/">
	<img src="/assets/images/logo.png" width="32" height="32" class="d-inline-block align-top" alt="Fei Qi">
	<span class="font-weight-bold">Fei</span> Qi</a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <!-- Begin Menu -->
      <div class="collapse navbar-collapse text-right" id="navbarNav">
	<ul class="navbar-nav ml-auto flex-nowrap">
	  <li class="nav-item"><a class="nav-link" href="/"><i class="fas fa-home"></i>&nbsp;About Me</a></li>
<li class="nav-item active"><a class="nav-link" href="/research/"><i class="fa fa-lightbulb"></i>&nbsp;Publications<span class="sr-only">(current)</span></a></li>
<li class="nav-item"><a class="nav-link" href="/teaching/"><i class="fa fa-tasks"></i>&nbsp;Teaching</a></li>
<li class="nav-item"><a class="nav-link" href="/blog/index.html"><i class="fa fa-rss-square"></i>&nbsp;Blog</a></li>
<li class="nav-item"><a class="nav-link" href="/sitemap.xml"><i class="fas fa-sitemap"></i>&nbsp;</a></li>

	</ul>
      </div><!-- End Menu -->
    </div>
  </nav>
</header>


    <!-- Content -->

    <div class="container my-3">
      <div class="post">

  <header class="header-bar">
    <h1 class="post-title">Publications</h1>    
    <p class="post-description">Generated by jekyll-scholar.</p>
  </header>

  <article>
    <h1 id="journal-papers">Journal Papers</h1>

<div class="publications">
<ol class="bibliography"><li><div class="row">
  <div class="col">
    <span class="author">Hao Li, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">A Novel Spatio-Temporal 3D Convolutional Encoder-Decoder Network for Dynamic Saliency Prediction.</span>
<span class="periodical"><em>IEEE Access,</em> 9:36328–36341,</span>
<span class="date"> Mar 2021.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-li_novel_2021')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-li_novel_2021')">Abs</button><a href="http://doi.org/10.1109/ACCESS.2021.3063372" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-li_novel_2021" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{li_novel_2021,
  title = {A Novel Spatio-Temporal 3D Convolutional Encoder-Decoder Network for Dynamic Saliency Prediction},
  author = {Li, Hao and Qi, Fei and Shi, Guangming},
  year = {2021},
  month = mar,
  volume = {9},
  pages = {36328--36341},
  doi = {10.1109/ACCESS.2021.3063372},
  journal = {IEEE Access}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-li_novel_2021" style="display: none;">
  <span class="abstract hidden">
    <p>As human beings are living in an always changing environment, predicting saliency maps from dynamic visual stimulus is of importance for modeling human visual system. Compared with human behavior, recent models based on LSTM and 3DCNN are still not good enough due to the limitation in spatio-temporal feature representation. In this paper, a novel 3D convolutional encoder-decoder architecture is proposed for saliency prediction on dynamic scenes. The encoder consists of two subnetworks to extract both spatial and temporal features in parallel with intermediate fusion, respectively. The saliency map is produced in decoder by firstly enlarging features in spatial dimensions and then aggregating temporal information. Specially designed structures can transfer pooling indices from encoder to decoder, which helps the generation of location-aware saliency maps. The proposed network can be trained and inferred in an end-to-end manner. Experimental results on benchmark DHF1K show that the proposed model achieves the state-of-the-art performance on key metrics including both normalized scanpath saliency and Pearson’s correlation coefficient.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Hao Li, Fei Qi, Guangming Shi, <!--and -->Chunhuan Lin.</span>
<span class="title">A Multiscale Dilated Dense Convolutional Network for Saliency Prediction with Instance-Level Attention Competition.</span>
<span class="periodical"><em>Journal of Visual Communication and Image Representation,</em> 64:102611,</span>
<span class="date"> Oct 2019.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-li_multiscale_2019')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-li_multiscale_2019')">Abs</button><a href="http://doi.org/10.1016/j.jvcir.2019.102611" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-li_multiscale_2019" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{li_multiscale_2019,
  title = {A {{Multiscale Dilated Dense Convolutional Network}} for {{Saliency Prediction}} with {{Instance}}-{{Level Attention Competition}}},
  author = {Li, Hao and Qi, Fei and Shi, Guangming and Lin, Chunhuan},
  year = {2019},
  month = oct,
  volume = {64},
  pages = {102611},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2019.102611},
  copyright = {All rights reserved},
  journal = {Journal of Visual Communication and Image Representation},
  keywords = {NSFC,saliency}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-li_multiscale_2019" style="display: none;">
  <span class="abstract hidden">
    <p>Data-driven saliency estimation attracts increasing interests in recent years because of the establishment of large-scale annotated datasets and the evolution of deep convolutional neural networks (CNN). Although CNN-based models perform much better than traditional ones in saliency prediction, there is still a gap between computational models and human behavior. One reason is that existing approaches fail assigning correct saliency to different objects in scenes with multiple objects. In this paper, we propose a multiscale dilated dense convolutional network to handle instance-level attention competition for better saliency prediction. In the proposed architecture, dense connections encode inter- and intra-class features for instance-level attention competition, dilated convolution collects contextual information to enrich feature representations of instances, and shortcut connections provide multiscale features for attention competition across scales. According to evaluations on three challenging datasets, CAT2000, SALICON, and MIT1003, the proposed model achieves the state-of-the-art performance.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Chunhuan Lin, Guangming Shi, <!--and -->Hao Li.</span>
<span class="title">A Convolutional Encoder-Decoder Network with Skip Connections for Saliency Prediction.</span>
<span class="periodical"><em>IEEE Access,</em> 7:60428–60438,</span>
<span class="date"> May 2019.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_convolutional_2019')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-qi_convolutional_2019')">Abs</button><a href="http://doi.org/10.1109/ACCESS.2019.2915630" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_convolutional_2019" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_convolutional_2019,
  title = {A {{Convolutional Encoder}}-{{Decoder Network}} with {{Skip Connections}} for {{Saliency Prediction}}},
  author = {Qi, Fei and Lin, Chunhuan and Shi, Guangming and Li, Hao},
  year = {2019},
  month = may,
  volume = {7},
  pages = {60428--60438},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2915630},
  journal = {IEEE Access},
  keywords = {NSFC,saliency}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-qi_convolutional_2019" style="display: none;">
  <span class="abstract hidden">
    <p>In this paper, we propose a novel convolutional encoder-decoder network with skip connections, named CEDNS, to improve the performance of saliency prediction. The encoder network utilizes the DenseNet model as the stem network to extract abundant hierarchical features from input images. Subsequently, a decoder network is designed to sufficiently fuse the hierarchical features to predict saliency more accurately. Between the encoder and decoder, skip connections are employed to transfer hierarchical features produced by the former to the latter. Furthermore, the model can be trained in an end-to-end manner which is beneficial for both training and inference. Experimental results on various benchmark datasets, SALICON, MIT300, and CAT2000, show that the proposed model achieves state-of-the-art performance on several key metrics.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Junwei Han, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning.</span>
<span class="periodical"><em>IEEE Transactions on Image Processing,</em> 28(7):3502–3515,</span>
<span class="date"> Jul 2019.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_predicting_2019')">BibTeX</button>
<a href="http://doi.org/10.1109/TIP.2019.2897966" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_predicting_2019" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{xia_predicting_2019,
  title = {Predicting {{Human Saccadic Scanpaths Based}} on {{Iterative Representation Learning}}},
  author = {Xia, Chen and Han, Junwei and Qi, Fei and Shi, Guangming},
  year = {2019},
  month = jul,
  volume = {28},
  pages = {3502--3515},
  doi = {10.1109/TIP.2019.2897966},
  journal = {IEEE Transactions on Image Processing},
  abbr = {IEEE TIP},
  keywords = {NSFC,saliency},
  number = {7}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Zhaohui Xia, Gaoyang Tang, Hang Yang, Yu Song, Guangrui Qian, Xiong An, Chunhuan Lin, <!--and -->Guangming Shi.</span>
<span class="title">DarwinML: A Graph-Based Evolutionary Algorithm for Automated Machine Learning.</span>
<span class="periodical"><em>arXiv:1901.08013 [cs.NE],</em></span>
<span class="date"> Nov 2018.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_darwinml:_2018')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-qi_darwinml:_2018')">Abs</button><a href="http://arxiv.org/abs/1901.08013" target="_blank">
  <button class="btn-arxiv"><i class="ai ai-arxiv"></i> arXiv: 1901.08013</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_darwinml:_2018" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_darwinml:_2018,
  title = {{{DarwinML}}: {{A Graph}}-Based {{Evolutionary Algorithm}} for {{Automated Machine Learning}}},
  shorttitle = {{{DarwinML}}},
  author = {Qi, Fei and Xia, Zhaohui and Tang, Gaoyang and Yang, Hang and Song, Yu and Qian, Guangrui and An, Xiong and Lin, Chunhuan and Shi, Guangming},
  year = {2018},
  month = nov,
  archiveprefix = {arXiv},
  eprint = {1901.08013},
  eprinttype = {arxiv},
  journal = {arXiv:1901.08013 [cs.NE]},
  primaryclass = {cs.NE}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-qi_darwinml:_2018" style="display: none;">
  <span class="abstract hidden">
    <p>As an emerging field, Automated Machine Learning (AutoML) aims to reduce or eliminate manual operations that require expertise in machine learning. In this paper, a graph-based architecture is employed to represent flexible combinations of ML models, which provides a large searching space compared to tree-based and stacking-based architectures. Based on this, an evolutionary algorithm is proposed to search for the best architecture, where the mutation and heredity operators are the key for architecture evolution. With Bayesian hyper-parameter optimization, the proposed approach can automate the workflow of machine learning. On the PMLB dataset, the proposed approach shows the state-of-the-art performance compared with TPOT, Autostacker, and auto-sklearn. Some of the optimized models are with complex structures which are difficult to obtain in manual design.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Fei Qi, Guangming Shi, <!--and -->Chunhuan Lin.</span>
<span class="title">Stereoscopic Saliency Estimation with Background Priors Based Deep Reconstruction.</span>
<span class="periodical"><em>Neurocomputing,</em> 321:126–138,</span>
<span class="date"> Dec 2018.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_stereoscopic_2018')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-xia_stereoscopic_2018')">Abs</button><a href="http://doi.org/10.1016/j.neucom.2018.09.009" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_stereoscopic_2018" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{xia_stereoscopic_2018,
  title = {Stereoscopic {{Saliency Estimation}} with {{Background Priors Based Deep Reconstruction}}},
  author = {Xia, Chen and Qi, Fei and Shi, Guangming and Lin, Chunhuan},
  year = {2018},
  month = dec,
  volume = {321},
  pages = {126--138},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.09.009},
  journal = {Neurocomputing},
  keywords = {NSFC,saliency}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-xia_stereoscopic_2018" style="display: none;">
  <span class="abstract hidden">
    <p>Studies have implied that depth is one of the important cues guiding visual attention. However, depth information has not been well explored in existing saliency estimation models. In this paper, we propose a model inspired by the observations in three-dimensional environment to better present the influence of depth. Firstly, we use depth to estimate each region’s probability of being background. Afterward, we sample pairs of surrounding and central patches from the possible background of each image to train an autoencoder-based network. As a network learned from background, it tends to describe the center-surround reconstruction pattern in background rather than foreground. Therefore, the detection of saliency can be formulated by measuring the reconstruction residual of the network. With emphasis on sampling from background, the proposed method can decrease the false positive rate in stereoscopic saliency estimation significantly. Experimental results demonstrate that the proposed method can outperform the state-of-the-art fixation prediction algorithms on several public data sets for stereoscopic saliency estimation. Additionally, it is efficiently used for proto-object extraction.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Xiaohu Yu, Artem R. Oganov, Qiang Zhu, Fei Qi, <!--and -->Guangrui Qian.</span>
<span class="title">The Stability and Unexpected Chemistry of Oxide Clusters.</span>
<span class="periodical"><em>Physical Chemistry Chemical Physics,</em> 20(48):30437–30444,</span>
<span class="date"> Dec 2018.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-yu_stability_2018')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-yu_stability_2018')">Abs</button><a href="http://doi.org/10.1039/C8CP03519A" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-yu_stability_2018" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{yu_stability_2018,
  title = {The Stability and Unexpected Chemistry of Oxide Clusters},
  author = {Yu, Xiaohu and Oganov, Artem R. and Zhu, Qiang and Qi, Fei and Qian, Guangrui},
  year = {2018},
  month = dec,
  volume = {20},
  pages = {30437--30444},
  issn = {1463-9084},
  doi = {10.1039/C8CP03519A},
  journal = {Physical Chemistry Chemical Physics},
  language = {en},
  number = {48}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-yu_stability_2018" style="display: none;">
  <span class="abstract hidden">
    <p>Using evolutionary structure prediction and ab initio thermodynamics, we determine stable compositions and structures of small CemOn and FemOn clusters at realistic temperatures and oxygen pressures. We use second energy differences as the criterion determining clusters of particular stability (“magic” clusters), whereas HOMO–LUMO gaps are used to gauge chemical inertness – i.e. the ability of a cluster to survive in a complex chemical environment. We find that, similar to atomic nuclei (which are clusters made of neutrons and protons), compositional space of two-component clusters also has ridges and islands of stability, surrounded by sea of instability. Long ridges of stability correspond to stoichiometric compositions – e.g., (CeO2)k, (Ce2O3)k, (FeO)k, (Fe2O3)k and (Fe3O4)k series of clusters, while “islands of stability” can have very unexpected compositions. For example, at room temperature and ambient atmosphere, superoxidized Fe4O8 clusters will be dominant among the Fe4On clusters. We emphasize that stability is dictated not only by closed geometric and electronic shells, but also by magnetism.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chunxiao Fan, Fu Li, Guangming Shi, Yi Niu, Fei Qi, Xuemei Xie, <!--and -->Dandan Jiao.</span>
<span class="title">A Hierarchical Multiplier-Free Architecture for HEVC Transform.</span>
<span class="periodical"><em>Multimedia Tools and Applications,</em> 76(1):997–1015,</span>
<span class="date"> Jan 2017.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-fan_hierarchical_2017')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-fan_hierarchical_2017')">Abs</button><a href="http://doi.org/10.1007/s11042-015-3114-3" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-fan_hierarchical_2017" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{fan_hierarchical_2017,
  title = {A Hierarchical Multiplier-Free Architecture for {{HEVC}} Transform},
  author = {Fan, Chunxiao and Li, Fu and Shi, Guangming and Niu, Yi and Qi, Fei and Xie, Xuemei and Jiao, Dandan},
  year = {2017},
  month = jan,
  volume = {76},
  pages = {997--1015},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-015-3114-3},
  journal = {Multimedia Tools and Applications},
  language = {en},
  number = {1}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-fan_hierarchical_2017" style="display: none;">
  <span class="abstract hidden">
    <p>In spite of high decorrelation performance, the large block size of transform coding in High Efficiency Video Coding (HEVC) brings about undesirable complexity in hardware design. The heaviest burden in HEVC transform implementation is the large quantity of multiplications. In this paper, we propose a novel hierarchical multiplier-free architecture for HEVC transform, which can achieve a multiplier-free partial butterfly combined with matrix multiplications (PBMM) architecture based on vector decomposition (VD-PBMM). In the proposed architecture, the complicate matrix multiplication in PBMM is achieved by several simple stages to simplify its VLSI realization. Each stage only involves additions and multiplications with power of two which can be achieved by shifters and adders. In addition, the new architecture can balance the distribution of adders to improve the system frequency. The proposed architecture has been evaluated with TSMC 0.13um CMOS technology. The relative system can run at 400 MHz with 92 K logic gates, which is about half of the PBMM method when the latency is 8. The proposed architecture can achieve the transform without any performance loss compared with the standard, and it is suitable for the hardware implementation in VLSI design.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">C. Fan, Y. Niu, G. Shi, F. Li, F. Qi, X. Xie, <!--and -->D. Jiao.</span>
<span class="title">An Improved Signed Digit Representation Approach for Constant Vector Multiplication.</span>
<span class="periodical"><em>IEEE Transactions on Circuits and Systems II: Express Briefs,</em> 63(10):999–1003,</span>
<span class="date"> Oct 2016.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-fan_improved_2016')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-fan_improved_2016')">Abs</button><a href="http://doi.org/10.1109/TCSII.2016.2539079" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-fan_improved_2016" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{fan_improved_2016,
  title = {An {{Improved Signed Digit Representation Approach}} for {{Constant Vector Multiplication}}},
  author = {Fan, C. and Niu, Y. and Shi, G. and Li, F. and Qi, F. and Xie, X. and Jiao, D.},
  year = {2016},
  month = oct,
  volume = {63},
  pages = {999--1003},
  issn = {1549-7747},
  doi = {10.1109/TCSII.2016.2539079},
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  keywords = {NSFC,saliency},
  number = {10}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-fan_improved_2016" style="display: none;">
  <span class="abstract hidden">
    <p>In this brief, the multiplier-free implementation of the constant vector multiplication is reexamined. A novel improved signed digit representation technique is proposed to overcome the two main drawbacks of the current multiplier-free techniques: 1) computational redundancy and 2) circuit irregularity. The fundamental difference between the proposed technique and the existing multiplier-free techniques is a novel optimization framework based on vector decomposition. The constant vector is decomposed into two terms: a “public” vector and a “private” matrix which consist of the public operations shared by all of the entries and the private operations of each individual entry, respectively. In this way, the overall data flow can be divided into two regular steps: multiplied by the “public” vector first and then by the “private” matrix. The computational complexity reduction task is then achieved by minimizing the length of the “public” vector and the number of operations in the “private” matrix. Experimental results demonstrate that the proposed technique outperforms the existing multiplier-free techniques in fewer operations and more regular circuit structure.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Bottom-Up Visual Saliency Estimation with Deep Autoencoder-Based Sparse Reconstruction.</span>
<span class="periodical"><em>IEEE Transactions on Neural Networks and Learning Systems,</em> 27(6):1227–1240,</span>
<span class="date"> Jun 2016.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_bottom-up_2016')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-xia_bottom-up_2016')">Abs</button><a href="http://doi.org/10.1109/TNNLS.2015.2512898" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_bottom-up_2016" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{xia_bottom-up_2016,
  title = {Bottom-{{Up Visual Saliency Estimation}} with {{Deep Autoencoder}}-Based {{Sparse Reconstruction}}},
  author = {Xia, Chen and Qi, Fei and Shi, Guangming},
  year = {2016},
  month = jun,
  volume = {27},
  pages = {1227--1240},
  publisher = {{masterpiece}},
  doi = {10.1109/TNNLS.2015.2512898},
  copyright = {All rights reserved},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {NSFC,saliency},
  number = {6}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-xia_bottom-up_2016" style="display: none;">
  <span class="abstract hidden">
    <p>Research on visual perception indicates that the human visual system is sensitive to center-surround (C-S) contrast in the bottom-up saliency-driven attention process. Different from the traditional contrast computation of feature difference, models based on reconstruction have emerged to estimate saliency by starting from original images themselves instead of seeking for certain ad hoc features. However, in the existing reconstruction-based methods, the reconstruction parameters of each area are calculated independently without taking their global correlation into account. In this paper, inspired by the powerful feature learning and data reconstruction ability of deep autoencoders, we construct a deep C-S inference network and train it with the data sampled randomly from the entire image to obtain a unified reconstruction pattern for the current image. In this way, global competition in sampling and learning processes can be integrated into the nonlocal reconstruction and saliency estimation of each pixel, which can achieve better detection results than the models with separate consideration on local and global rarity. Moreover, by learning from the current scene, the proposed model can achieve the feature extraction and interaction simultaneously in an adaptive way, which can form a better generalization ability to handle more types of stimuli. Experimental results show that in accordance with different inputs, the network can learn distinct basic features for saliency modeling in its code layer. Furthermore, in a comprehensive evaluation on several benchmark data sets, the proposed method can outperform the existing state-of-the-art algorithms.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Fei Qi, Guangming Shi, <!--and -->Pengjin Wang.</span>
<span class="title">Nonlocal Center–Surround Reconstruction-Based Bottom-up Saliency Estimation.</span>
<span class="periodical"><em>Pattern Recognition,</em> 48(4):1337–1348,</span>
<span class="date"> Apr 2015.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_nonlocal_2015')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-xia_nonlocal_2015')">Abs</button><a href="http://doi.org/10.1016/j.patcog.2014.10.007" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_nonlocal_2015" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{xia_nonlocal_2015,
  title = {Nonlocal Center\textendash Surround Reconstruction-Based Bottom-up Saliency Estimation},
  author = {Xia, Chen and Qi, Fei and Shi, Guangming and Wang, Pengjin},
  year = {2015},
  month = apr,
  volume = {48},
  pages = {1337--1348},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2014.10.007},
  copyright = {All rights reserved},
  journal = {Pattern Recognition},
  keywords = {publication},
  number = {4}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-xia_nonlocal_2015" style="display: none;">
  <span class="abstract hidden">
    <p>Many saliency models consider the feature extraction as the algorithmic core and the performance of their methods relies on the selection of the features to a great extent. However, there can hardly be a set of features effective to pop out the salient regions under various visual environments. Moreover, because saliency is not tuned to certain visual features, a location winning the spatial competition in any feature space can be defined as salient. Instead of seeking for or learning the features to highlight the difference between the salient areas and the background, we focus more on the sparsity and uniqueness carried by the original image itself, the source of all the features, to propose a nonlocal reconstruction-based saliency model. In the proposed approach, the saliency is measured by the sparse reconstruction residual of representing the central patch with a linear combination of its surrounding patches sampled in a nonlocal manner. In addition, this is generalized to model the global aspect saliency, which provides a complement to the nonlocal saliency and improves the performance further. As a generalization of Itti et al.׳s classical center–surround comparison scheme, the proposed approach performs well on images where Itti et al.׳s method fails, as well as on general natural images. Numerical experiments show the proposed approach produces better results compared with the state-of-the-art algorithms on three public databases.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chang Wang, Fei Qi, Guangming Shi, <!--and -->Xiaotian Wang.</span>
<span class="title">Convex Combination Based Target Localization with Noisy Angle of Arrival Measurements.</span>
<span class="periodical"><em>IEEE Wireless Communications Letters,</em> 3(1):14–17,</span>
<span class="date"> Feb 2014.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_convex_2014')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_convex_2014')">Abs</button><a href="http://doi.org/10.1109/WCL.2013.101613.130587" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wang_convex_2014" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wang_convex_2014,
  title = {Convex {{Combination Based Target Localization}} with {{Noisy Angle}} of {{Arrival Measurements}}},
  author = {Wang, Chang and Qi, Fei and Shi, Guangming and Wang, Xiaotian},
  year = {2014},
  month = feb,
  volume = {3},
  pages = {14--17},
  doi = {10.1109/WCL.2013.101613.130587},
  copyright = {All rights reserved},
  journal = {IEEE Wireless Communications Letters},
  keywords = {publication},
  number = {1}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_convex_2014" style="display: none;">
  <span class="abstract hidden">
    <p>Target localization based on angle of arrival (AoA) is an important branch of location estimation research. Due to the noisy measurements from sensors and the non-linear inverse trigonometry function, the AoA-based localization induces a non-convex optimization that is difficult to solve with both high speed and accuracy simultaneously. To achieve a fast and accurate algorithm, we propose a convex combination scheme. By introducing a highly accurate linear approximation to the inverse trigonometric function, the objective is converted to a convex function, which can be solved efficiently with linear least-squares approach. The key point of the convex combination scheme is to express the coordinate of the target as the convex combination of a set of virtual anchors around its real position. Simulations demonstrate that the accuracy of this method is very close to the Cramer-Rao lower bound and that the comprehensive performance, including the accuracy and speed, is significantly improved compared to other state-of-the-art methods.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chang Wang, Fei Qi, Guangming Shi, <!--and -->Jingbo Ren.</span>
<span class="title">A Linear Combination-Based Weighted Least Square Approach for Target Localization with Noisy Range Measurements.</span>
<span class="periodical"><em>Signal Processing,</em> 94:202–211,</span>
<span class="date"> Jan 2014.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_linear_2014')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_linear_2014')">Abs</button><a href="http://doi.org/10.1016/j.sigpro.2013.06.005" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wang_linear_2014" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wang_linear_2014,
  title = {A Linear Combination-Based Weighted Least Square Approach for Target Localization with Noisy Range Measurements},
  author = {Wang, Chang and Qi, Fei and Shi, Guangming and Ren, Jingbo},
  year = {2014},
  month = jan,
  volume = {94},
  pages = {202--211},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2013.06.005},
  copyright = {All rights reserved},
  journal = {Signal Processing},
  keywords = {publication}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_linear_2014" style="display: none;">
  <span class="abstract hidden">
    <p>Abstract
Target localization based on range measurements from a set of anchors plays an important role in positioning systems and sensor networks. The localization is generally formulated as an optimization problem to tackle the noisy measurements. However, the objective is non-convex, and thus localization is difficult to solve in its original form. In this paper, a convex objective function is derived based on a linear combination scheme, within which the target position is expressed as a linear combination of positions of virtual anchors around its real position. In addition, the linear combination provides a highly accurate approximation for the computation of the distance from the anchors. Thus, the localization is formulated as a convex problem to find the optimal coefficients of the linear combination and is solved efficiently by the weighted linear least square method. As demonstrated by numerical experiments, the proposed approach, which achieves an approximately 35% improvement in accuracy and 98.5% shorter optimization time compared to the most accurate existing method, is very close to the Cramér–Rao lower bound (CRLB) while maintaining a quite high localization speed, and also works well with real measurement data.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Junyu Han, Pengjin Wang, Guangming Shi, <!--and -->Fu Li.</span>
<span class="title">Structure Guided Fusion for Depth Map Inpainting.</span>
<span class="periodical"><em>Pattern Recognition Letters,</em> 34(1):70–76,</span>
<span class="date"> Jan 2013.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_structure_2013')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-qi_structure_2013')">Abs</button><a href="http://doi.org/10.1016/j.patrec.2012.06.003" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_structure_2013" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_structure_2013,
  title = {Structure Guided Fusion for Depth Map Inpainting},
  author = {Qi, Fei and Han, Junyu and Wang, Pengjin and Shi, Guangming and Li, Fu},
  year = {2013},
  month = jan,
  volume = {34},
  pages = {70--76},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2012.06.003},
  copyright = {All rights reserved},
  journal = {Pattern Recognition Letters},
  keywords = {publication},
  number = {1}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-qi_structure_2013" style="display: none;">
  <span class="abstract hidden">
    <p>Depth acquisition becomes inexpensive after the revolutionary invention of Kinect. For computer vision applications, depth maps captured by Kinect require additional processing to fill up missing parts. However, conventional inpainting methods for color images cannot be applied directly to depth maps as there are not enough cues to make accurate inference about scene structures. In this paper, we propose a novel fusion based inpainting method to improve depth maps. The proposed fusion strategy integrates conventional inpainting with the recently developed non-local filtering scheme. The good balance between depth and color information guarantees an accurate inpainting result. Experimental results show the mean absolute error of the proposed method is about 20 mm, which is comparable to the precision of the Kinect sensor.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chang Wang, Fei Qi, Guangming Shi, <!--and -->Xiaotian Wang.</span>
<span class="title">A Sparse Representation-Based Deployment Method for Optimizing the Observation Quality of Camera Networks.</span>
<span class="periodical"><em>Sensors,</em> 13(9):11453–11475,</span>
<span class="date"> Aug 2013.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_sparse_2013')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_sparse_2013')">Abs</button><a href="http://doi.org/10.3390/s130911453" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wang_sparse_2013" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wang_sparse_2013,
  title = {A {{Sparse Representation}}-{{Based Deployment Method}} for {{Optimizing}} the {{Observation Quality}} of {{Camera Networks}}},
  author = {Wang, Chang and Qi, Fei and Shi, Guangming and Wang, Xiaotian},
  year = {2013},
  month = aug,
  volume = {13},
  pages = {11453--11475},
  doi = {10.3390/s130911453},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  journal = {Sensors},
  keywords = {publication},
  language = {en},
  number = {9}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_sparse_2013" style="display: none;">
  <span class="abstract hidden">
    <p>Deployment is a critical issue affecting the quality of service of camera networks.  The deployment aims at adopting the least number of cameras to cover the whole scene,  which may have obstacles to occlude the line of sight, with expected observation quality.  This is generally formulated as a non-convex optimization problem, which is hard to solve  in polynomial time. In this paper, we propose an efficient convex solution for deployment  optimizing the observation quality based on a novel anisotropic sensing model of cameras,  which provides a reliable measurement of the observation quality. The deployment is  formulated as the selection of a subset of nodes from a redundant initial deployment with  numerous cameras, which is an \mathscrl0 minimization problem. Then, we relax this non-convex  optimization to a convex \mathscrl1 minimization employing the sparse representation. Therefore,  the high quality deployment is efficiently obtained via convex optimization. Simulation  results confirm the effectiveness of the proposed camera deployment algorithms.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Guangming Shi, Weisi Lin, Anmin Liu, <!--and -->Fei Qi.</span>
<span class="title">Just Noticeable Difference Estimation for Images With Free-Energy Principle.</span>
<span class="periodical"><em>IEEE Transactions on Multimedia,</em> 15(7):1705–1710,</span>
<span class="date"> Nov 2013.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_just_2013')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wu_just_2013')">Abs</button><a href="http://doi.org/10.1109/TMM.2013.2268053" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wu_just_2013" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wu_just_2013,
  title = {Just {{Noticeable Difference Estimation}} for {{Images With Free}}-{{Energy Principle}}},
  author = {Wu, Jinjian and Shi, Guangming and Lin, Weisi and Liu, Anmin and Qi, Fei},
  year = {2013},
  month = nov,
  volume = {15},
  pages = {1705--1710},
  issn = {1520-9210},
  doi = {10.1109/TMM.2013.2268053},
  journal = {IEEE Transactions on Multimedia},
  number = {7}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wu_just_2013" style="display: none;">
  <span class="abstract hidden">
    <p>In this paper, we introduce a novel just noticeable difference (JND) estimation model based on the unified brain theory, namely the free-energy principle. The existing pixel-based JND models mainly consider the orderly factors and always underestimate the JND threshold of the disorderly region. Recent research indicates that the human visual system (HVS) actively predicts the orderly information and avoids the residual disorderly uncertainty for image perception and understanding. Thus, we suggest that there exists disorderly concealment effect which results in high JND threshold of the disorderly region. Beginning with the Bayesian inference, we deduce an autoregressive model to imitate the active prediction of the HVS. Then, we estimate the disorderly concealment effect for the novel JND model. Experimental results confirm that the proposed JND model outperforms the relevant existing ones. Furthermore, we apply the proposed JND model in image compression, and around 15% of bit rate can be reduced without jeopardizing the perceptual quality.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jie Lin, Guangming Shi, Xuyang Chen, Fei Qi, Li Zhang, <!--and -->Xuemei Xie.</span>
<span class="title">High-Resolution Ranging Method Based on Low-Rate Parallel Random Sampling.</span>
<span class="periodical"><em>Digital Signal Processing,</em> 22(2):219–225,</span>
<span class="date"> Mar 2012.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-lin_high-resolution_2012')">BibTeX</button>
<a href="http://doi.org/10.1016/j.dsp.2011.11.006" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-lin_high-resolution_2012" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{lin_high-resolution_2012,
  title = {High-Resolution Ranging Method Based on Low-Rate Parallel Random Sampling},
  author = {Lin, Jie and Shi, Guangming and Chen, Xuyang and Qi, Fei and Zhang, Li and Xie, Xuemei},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {219--225},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2011.11.006},
  copyright = {All rights reserved},
  journal = {Digital Signal Processing},
  keywords = {publication},
  number = {2}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Fei Qi, Guangming Shi, <!--and -->Yongheng Lu.</span>
<span class="title">Non-Local Spatial Redundancy Reduction for Bottom-up Saliency Estimation.</span>
<span class="periodical"><em>Journal of Visual Communication and Image Representation,</em> 23(7):1158–1166,</span>
<span class="date"> Oct 2012.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_non-local_2012')">BibTeX</button>
<a href="http://doi.org/10.1016/j.jvcir.2012.07.010" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wu_non-local_2012" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wu_non-local_2012,
  title = {Non-Local Spatial Redundancy Reduction for Bottom-up Saliency Estimation},
  author = {Wu, Jinjian and Qi, Fei and Shi, Guangming and Lu, Yongheng},
  year = {2012},
  month = oct,
  volume = {23},
  pages = {1158--1166},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2012.07.010},
  copyright = {All rights reserved},
  journal = {Journal of Visual Communication and Image Representation},
  keywords = {publication},
  lccn = {0001},
  number = {7}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Self-Similarity Based Structural Regularity for Just Noticeable Difference Estimation.</span>
<span class="periodical"><em>Journal of Visual Communication and Image Representation,</em> 23(6):845–852,</span>
<span class="date"> Aug 2012.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_self-similarity_2012')">BibTeX</button>
<a href="http://doi.org/10.1016/j.jvcir.2012.04.010" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wu_self-similarity_2012" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{wu_self-similarity_2012,
  title = {Self-Similarity Based Structural Regularity for Just Noticeable Difference Estimation},
  author = {Wu, Jinjian and Qi, Fei and Shi, Guangming},
  year = {2012},
  month = aug,
  volume = {23},
  pages = {845--852},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2012.04.010},
  copyright = {All rights reserved},
  journal = {Journal of Visual Communication and Image Representation},
  keywords = {publication},
  lccn = {0000},
  number = {6}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fu Li, Guangming Shi, Fei Qi, <!--and -->Li Zhang.</span>
<span class="title">Bandwidth Adaption for Kernel Particle Filter.</span>
<span class="periodical"><em>Journal of Systems Engineering and Electronics,</em> 22(2):340–346,</span>
<span class="date"> Apr 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-li_bandwidth_2011')">BibTeX</button>
<a href="http://doi.org/10.3969/j.issn.1004-4132.2011.02.023" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-li_bandwidth_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{li_bandwidth_2011,
  title = {Bandwidth Adaption for Kernel Particle Filter},
  author = {Li, Fu and Shi, Guangming and Qi, Fei and Zhang, Li},
  year = {2011},
  month = apr,
  volume = {22},
  pages = {340--346},
  doi = {10.3969/j.issn.1004-4132.2011.02.023},
  journal = {Journal of Systems Engineering and Electronics},
  keywords = {publication},
  number = {2}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Guangming Shi, Xuyang Chen, Xiaoxia Song, Fei Qi, <!--and -->Ailing Ding.</span>
<span class="title">Signal Matching Wavelet for Ultrasonic Flaw Detection in High Background Noise.</span>
<span class="periodical"><em>IEEE Transanctions on Ultrasonics, Ferroelectrics and Frequency Control,</em> 58(4):776–787,</span>
<span class="date"> Apr 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-shi_signal_2011')">BibTeX</button>
<a href="http://doi.org/10.1109/TUFFC.2011.1870" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-shi_signal_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{shi_signal_2011,
  title = {Signal Matching Wavelet for Ultrasonic Flaw Detection in High Background Noise},
  author = {Shi, Guangming and Chen, Xuyang and Song, Xiaoxia and Qi, Fei and Ding, Ailing},
  year = {2011},
  month = apr,
  volume = {58},
  pages = {776--787},
  issn = {0885-3010},
  doi = {10.1109/TUFFC.2011.1870},
  copyright = {All rights reserved},
  journal = {IEEE Transanctions on Ultrasonics, Ferroelectrics and Frequency Control},
  keywords = {publication},
  number = {4}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fu Li, Fei Qi, Guangming Shi, <!--and -->Li Zhang.</span>
<span class="title">Optimization-Based Particle Filter for State and Parameter Estimation.</span>
<span class="periodical"><em>Journal of Systems Engineering and Electronics,</em> 20(3):479–484,</span>
<span class="date"> Jun 2009.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-li_optimization-based_2009')">BibTeX</button>

<!-- Hidden BibTeX block -->
<div id="bib-li_optimization-based_2009" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{li_optimization-based_2009,
  title = {Optimization-Based Particle Filter for State and Parameter Estimation},
  author = {Li, Fu and Qi, Fei and Shi, Guangming and Zhang, Li},
  year = {2009},
  month = jun,
  volume = {20},
  pages = {479--484},
  journal = {Journal of Systems Engineering and Electronics},
  keywords = {filter,particle,publication},
  number = {3}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Guangming Shi, Jie Lin, Xuyang Chen, Fei Qi, Danhua Liu, <!--and -->L. Zhang.</span>
<span class="title">UWB Echo Signal Detection With Ultra-Low Rate Sampling Based on Compressed Sensing.</span>
<span class="periodical"><em>IEEE Transactions on Circuits and Systems II: Express Briefs,</em> 55(4):379–383,</span>
<span class="date"> Apr 2008.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-shi_uwb_2008')">BibTeX</button>
<a href="http://doi.org/10.1109/TCSII.2008.918988" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-shi_uwb_2008" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{shi_uwb_2008,
  title = {{{UWB Echo Signal Detection With Ultra}}-{{Low Rate Sampling Based}} on {{Compressed Sensing}}},
  author = {Shi, Guangming and Lin, Jie and Chen, Xuyang and Qi, Fei and Liu, Danhua and Zhang, L.},
  year = {2008},
  month = apr,
  volume = {55},
  pages = {379--383},
  issn = {1549-7747},
  doi = {10.1109/TCSII.2008.918988},
  copyright = {All rights reserved},
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  keywords = {publication},
  number = {4}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Qihe Li, Yupin Luo, <!--and -->Dongcheng Hu.</span>
<span class="title">Camera Calibration with One-Dimensional Objects Moving under Gravity.</span>
<span class="periodical"><em>Pattern Recognition,</em> 40(1):343–345,</span>
<span class="date"> Jan 2007.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_camera_2007')">BibTeX</button>
<a href="http://doi.org/10.1016/j.patcog.2006.06.029" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_camera_2007" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_camera_2007,
  title = {Camera Calibration with One-Dimensional Objects Moving under Gravity},
  author = {Qi, Fei and Li, Qihe and Luo, Yupin and Hu, Dongcheng},
  year = {2007},
  month = jan,
  volume = {40},
  pages = {343--345},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2006.06.029},
  copyright = {All rights reserved},
  journal = {Pattern Recognition},
  keywords = {publication},
  number = {1}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Qihe Li, Yupin Luo, <!--and -->Dongcheng Hu.</span>
<span class="title">Constraints on General Motions for Camera Calibration with One-Dimensional Objects.</span>
<span class="periodical"><em>Pattern Recognition,</em> 40(6):1785–1792,</span>
<span class="date"> Jun 2007.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_constraints_2007')">BibTeX</button>
<a href="http://doi.org/10.1016/j.patcog.2006.11.001" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_constraints_2007" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_constraints_2007,
  title = {Constraints on General Motions for Camera Calibration with One-Dimensional Objects},
  author = {Qi, Fei and Li, Qihe and Luo, Yupin and Hu, Dongcheng},
  year = {2007},
  month = jun,
  volume = {40},
  pages = {1785--1792},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2006.11.001},
  copyright = {All rights reserved},
  journal = {Pattern Recognition},
  keywords = {publication},
  number = {6}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Yupin Luo, <!--and -->Dongcheng Hu.</span>
<span class="title">Recognition of Perspectively Distorted Planar Grids.</span>
<span class="periodical"><em>Pattern Recognition Letters,</em> 27(14):1725–1731,</span>
<span class="date"> Oct 2006.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_recognition_2006')">BibTeX</button>
<a href="http://doi.org/10.1016/j.patrec.2006.04.014" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_recognition_2006" style="display: none;">
  <span class="bibtex hidden">
    <pre>@article{qi_recognition_2006,
  title = {Recognition of Perspectively Distorted Planar Grids},
  author = {Qi, Fei and Luo, Yupin and Hu, Dongcheng},
  year = {2006},
  month = oct,
  volume = {27},
  pages = {1725--1731},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2006.04.014},
  copyright = {All rights reserved},
  journal = {Pattern Recognition Letters},
  keywords = {publication},
  number = {14}
}
</pre>
  </span>
</div>


  </div>
</div>
</li></ol>
</div>

<h1 id="conference-papers">Conference Papers</h1>

<div class="publications">
<ol class="bibliography"><li><div class="row">
  <div class="col">
    <span class="author">Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang, Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, <!--and -->Guangming Shi.</span>
<span class="title">PGNet: Real-Time Arbitrarily-Shaped Text Spotting with Point Gathering Network.</span>
<span class="periodical"><em>In Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI),</em></span>
<span class="date"> Feb 2021.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_pgnet_2021')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_pgnet_2021')">Abs</button>
<!-- Hidden BibTeX block -->
<div id="bib-wang_pgnet_2021" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wang_pgnet_2021,
  title = {{{PGNet}}: Real-Time Arbitrarily-Shaped Text Spotting with Point Gathering Network},
  booktitle = {Thirty-Fifth AAAI Conference on Artificial Intelligence ({{AAAI}})},
  author = {Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Liu, Shanshan and Zhang, Xiaoqiang and Lyu, Pengyuan and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming},
  year = {2021},
  month = feb
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_pgnet_2021" style="display: none;">
  <span class="abstract hidden">
    <p>The reading of arbitrarily-shaped text has received increasing research attention, but existing text spotters are mostly built on two-stage frameworks or character-based methods, which may suffer from character-level annotations, Non-Maximum Suppression (NMS) and Region-of-Interest (RoI) operations. In this paper, we attempt to address those problems and propose a novel point gathering network for reading arbitrarily-shaped text in real-time, named PGNet, and it is a single-shot text spotter in fully convolutional manner, and the pixel-level character classification map is learned with proposed PG-CTC loss avoiding using character-level annotations. With PG-CTC decoder, we gather high-level character classification vectors from two dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees the great efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and further improve the end-to-end performance. Experiments demonstrate that the proposed method achieves state-of-the-art or competitive accuracy, meanwhile significantly improving the running speed. In particular, on Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Pengfei Wang, Chengquan Zhang, Fei Qi, Zuming Huang, Mengyi En, Junyu Han, Jingtuo Liu, Errui Ding, <!--and -->Guangming Shi.</span>
<span class="title">A Single-Shot Arbitrarily-Shaped Text Detector Based on Context Attended Multi-Task Learning.</span>
<span class="periodical"><em>In Proceedings of the 27th ACM International Conference on Multimedia (MM ’19),</em></span>
<span class="date"> Oct 2019.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_single-shot_2019')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_single-shot_2019')">Abs</button><a href="http://doi.org/10.1145/3343031.3350988" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a><a href="http://arxiv.org/abs/1908.05498" target="_blank">
  <button class="btn-arxiv"><i class="ai ai-arxiv"></i> arXiv: 1908.05498</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wang_single-shot_2019" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wang_single-shot_2019,
  title = {A {{Single}}-{{Shot Arbitrarily}}-{{Shaped Text Detector}} Based on {{Context Attended Multi}}-{{Task Learning}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Multimedia}} ({{MM}} '19)},
  author = {Wang, Pengfei and Zhang, Chengquan and Qi, Fei and Huang, Zuming and En, Mengyi and Han, Junyu and Liu, Jingtuo and Ding, Errui and Shi, Guangming},
  year = {2019},
  month = oct,
  publisher = {{ACM}},
  address = {{Nice, France}},
  doi = {10.1145/3343031.3350988},
  archiveprefix = {arXiv},
  eprint = {1908.05498},
  eprinttype = {arxiv},
  keywords = {NSFC,saliency}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_single-shot_2019" style="display: none;">
  <span class="abstract hidden">
    <p>Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">An Iterative Representation Learning Framework to Predict the Sequence of Eye Fixations.</span>
<span class="periodical"><em>In Proceedings of the IEEE International Conference on Multimedia and Expo (ICME),</em> 1530–1535, </span>
<span class="date"> Jul 2017.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_iterative_2017')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-xia_iterative_2017')">Abs</button><a href="http://doi.org/10.1109/ICME.2017.8019396" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_iterative_2017" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{xia_iterative_2017,
  title = {An {{Iterative Representation Learning Framework}} to {{Predict}} the {{Sequence}} of {{Eye Fixations}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Xia, Chen and Qi, Fei and Shi, Guangming},
  year = {2017},
  month = jul,
  pages = {1530--1535},
  publisher = {{IEEE}},
  address = {{Hong Kong}},
  doi = {10.1109/ICME.2017.8019396},
  keywords = {NSFC,saliency}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-xia_iterative_2017" style="display: none;">
  <span class="abstract hidden">
    <p>Visual attention is a dynamic search process of acquiring information. However, most previous studies have focused on the prediction of static attended locations. Without considering the temporal relationship of fixations, these models usually cannot explain the dynamic saccadic behavior well. In this paper, an iterative representation learning framework is proposed to predict the saccadic scanpath. Within the proposed framework, saccade can be explained as an iterative process of finding the most uncertain area and updating the representation of scenes. In implementation, a deep autoencoder is employed for representation learning. The current fixation is predicted to be the most salient pixel, with saliency estimated by the reconstruction residual of the deep network. Image patches around this fixation are then sampled to update the network for the selection of subsequent fixations. Compared with existing models, the proposed model shows the state-of-the-art performance on several public data sets.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chen Xia, Pengjin Wang, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Nonlocal Center-Surround Reconstruction-Based Bottom-up Saliency Estimation.</span>
<span class="periodical"><em>In 2013 20th IEEE International Conference on Image Processing (ICIP),</em> 206–210, </span>
<span class="date"> Sep 2013.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-xia_nonlocal_2013')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-xia_nonlocal_2013')">Abs</button><a href="http://doi.org/10.1109/ICIP.2013.6738043" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-xia_nonlocal_2013" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{xia_nonlocal_2013,
  title = {Nonlocal Center-Surround Reconstruction-Based Bottom-up Saliency Estimation},
  booktitle = {2013 20th {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Xia, Chen and Wang, Pengjin and Qi, Fei and Shi, Guangming},
  year = {2013},
  month = sep,
  pages = {206--210},
  doi = {10.1109/ICIP.2013.6738043}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-xia_nonlocal_2013" style="display: none;">
  <span class="abstract hidden">
    <p>The center-surround comparison principle is widely used in existing bottom-up saliency estimation models. However, most of them are based on local image processing techniques which are hard to handle texture regions well as a relatively large neighborhood is required to represent textures. In this paper, we propose a nonlocal patch-based reconstruction approach to reformulate the center-surround comparison. In the proposed approach, the saliency is measured by the reconstruction residual of representing the central patch with a linear combination of its surrounding patches. As a generalization of Itti et al.’s classical center-surround comparison scheme, the proposed approach performs well on images with symmetric structures where Itti et al.’s method fails, as well as on general natural images. Numerical experiments show the proposed approach produces better results compared to the state-of-the-art algorithms on several public databases.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Image Quality Assessment Based on Improved Structural SIMilarity.</span>
<span class="periodical"><em>In Advances in Multimedia Information Processing — PCM,</em> 153–163, </span>
<span class="date"> Jan 2012.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_image_2012')">BibTeX</button>

<!-- Hidden BibTeX block -->
<div id="bib-wu_image_2012" style="display: none;">
  <span class="bibtex hidden">
    <pre>@incollection{wu_image_2012,
  title = {Image {{Quality Assessment Based}} on {{Improved Structural SIMilarity}}},
  booktitle = {Advances in {{Multimedia Information Processing}} \textemdash{} {{PCM}}},
  author = {Wu, Jinjian and Qi, Fei and Shi, Guangming},
  editor = {Lin, Weisi and Xu, Dong and Ho, Anthony and Wu, Jianxin and He, Ying and Cai, Jianfei and Kankanhalli, Mohan and Sun, Ming-Ting},
  year = {2012},
  month = jan,
  pages = {153--163},
  publisher = {{Springer B. H.}},
  isbn = {978-3-642-34777-1 978-3-642-34778-8},
  keywords = {publication},
  number = {7674},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Junyu Han, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Enhancing Gradient Sparsity for Parametrized Motion Estimation.</span>
<span class="periodical"><em>In Proc. British Machine Vision Conf.,</em> 42.1–42.0, </span>
<span class="date"> 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-han_enhancing_2011')">BibTeX</button>
<a href="http://doi.org/10.5244/C.25.42" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-han_enhancing_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{han_enhancing_2011,
  title = {Enhancing {{Gradient Sparsity}} for {{Parametrized Motion Estimation}}},
  booktitle = {Proc. {{British Machine Vision Conf}}.},
  author = {Han, Junyu and Qi, Fei and Shi, Guangming},
  year = {2011},
  pages = {42.1--42.0},
  publisher = {{BMVA Press}},
  address = {{Dundee, UK}},
  doi = {10.5244/C.25.42},
  isbn = {1-901725-43-X},
  keywords = {publication},
  lccn = {0000}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Junyu Han, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Gradient Sparsity for Piecewise Continuous Optical Flow Estimation.</span>
<span class="periodical"><em>In IEEE Int’l Conf. Image Processing (ICIP),</em> 2341–2344, </span>
<span class="date"> Sep 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-han_gradient_2011')">BibTeX</button>
<a href="http://doi.org/10.1109/ICIP.2011.6116110" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-han_gradient_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{han_gradient_2011,
  title = {Gradient Sparsity for Piecewise Continuous Optical Flow Estimation},
  booktitle = {{{IEEE Int}}'l {{Conf}}. {{Image Processing}} ({{ICIP}})},
  author = {Han, Junyu and Qi, Fei and Shi, Guangming},
  year = {2011},
  month = sep,
  pages = {2341--2344},
  publisher = {{IEEE}},
  address = {{Brussels, Belgium}},
  doi = {10.1109/ICIP.2011.6116110},
  isbn = {978-1-4577-1304-0},
  keywords = {publication},
  lccn = {0000}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chang Wang, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Observation Quality Guaranteed Layout of Camera Networks via Sparse Representation.</span>
<span class="periodical"><em>In IEEE Visual Communications and Image Processing (VCIP),</em></span>
<span class="date"> Nov 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_observation_2011')">BibTeX</button>
<a href="http://doi.org/10.1109/VCIP.2011.6116043" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wang_observation_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wang_observation_2011,
  title = {Observation Quality Guaranteed Layout of Camera Networks via Sparse Representation},
  booktitle = {{{IEEE Visual Communications}} and {{Image Processing}} ({{VCIP}})},
  author = {Wang, Chang and Qi, Fei and Shi, Guangming},
  year = {2011},
  month = nov,
  publisher = {{IEEE}},
  doi = {10.1109/VCIP.2011.6116043},
  isbn = {978-1-4577-1321-7},
  keywords = {publication},
  lccn = {0000}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">Unified Spatial Masking for Just-Noticeable Difference Estimation.</span>
<span class="periodical"><em>In Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),</em></span>
<span class="date"> Oct 2011.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_unified_2011')">BibTeX</button>

<!-- Hidden BibTeX block -->
<div id="bib-wu_unified_2011" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wu_unified_2011,
  title = {Unified {{Spatial Masking}} for {{Just}}-{{Noticeable Difference Estimation}}},
  booktitle = {Asia-{{Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  author = {Wu, Jinjian and Qi, Fei and Shi, Guangming},
  year = {2011},
  month = oct,
  address = {{Xi'an, China}},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Jinjian Wu, Fei Qi, <!--and -->Guangming Shi.</span>
<span class="title">An Improved Model of Pixel Adaptive Just-Noticeable Difference Estimation.</span>
<span class="periodical"><em>In IEEE Int’l Conf. Acoustics Speech and Signal Processing (ICASSP),</em> 2454–2457, </span>
<span class="date"> Mar 2010.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wu_improved_2010')">BibTeX</button>
<a href="http://doi.org/10.1109/ICASSP.2010.5496302" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-wu_improved_2010" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wu_improved_2010,
  title = {An Improved Model of Pixel Adaptive Just-Noticeable Difference Estimation},
  booktitle = {{{IEEE Int}}'l {{Conf}}. {{Acoustics Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wu, Jinjian and Qi, Fei and Shi, Guangming},
  year = {2010},
  month = mar,
  pages = {2454--2457},
  publisher = {{IEEE}},
  address = {{Dallas, Texas, USA}},
  doi = {10.1109/ICASSP.2010.5496302},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Jinjian Wu, <!--and -->Guangming Shi.</span>
<span class="title">Extracting Regions of Attention by Imitating the Human Visual System.</span>
<span class="periodical"><em>In IEEE Int’l Conf. Acoustics, Speech and Signal Processing (ICASSP),</em> 1905–1908, </span>
<span class="date"> Apr 2009.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_extracting_2009')">BibTeX</button>
<a href="http://doi.org/10.1109/ICASSP.2009.4959981" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_extracting_2009" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{qi_extracting_2009,
  title = {Extracting Regions of Attention by Imitating the Human Visual System},
  booktitle = {{{IEEE Int}}'l {{Conf}}. {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Qi, Fei and Wu, Jinjian and Shi, Guangming},
  year = {2009},
  month = apr,
  pages = {1905--1908},
  publisher = {{IEEE Press}},
  doi = {10.1109/ICASSP.2009.4959981},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Xiaowei Song, <!--and -->Guangming Shi.</span>
<span class="title">LDA Based Color Information Fusion for Visual Objects Tracking.</span>
<span class="periodical"><em>In IEEE Int’l Conf. Image Processing (ICIP),</em> 2201–2204, </span>
<span class="date"> Nov 2009.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_lda_2009')">BibTeX</button>
<a href="http://doi.org/10.1109/ICIP.2009.5413870" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_lda_2009" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{qi_lda_2009,
  title = {{{LDA}} Based Color Information Fusion for Visual Objects Tracking},
  booktitle = {{{IEEE Int}}'l {{Conf}}. {{Image Processing}} ({{ICIP}})},
  author = {Qi, Fei and Song, Xiaowei and Shi, Guangming},
  year = {2009},
  month = nov,
  pages = {2201--2204},
  publisher = {{IEEE Press}},
  address = {{Cairo, Egypt}},
  doi = {10.1109/ICIP.2009.5413870},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Chang Wang, Fei Qi, <!--and -->Guang-Ming Shi.</span>
<span class="title">Nodes Placement for Optimizing Coverage of Visual Sensor Networks.</span>
<span class="periodical"><em>In Advances in Multimedia Information Processing (PCM),</em> 1144–1149, </span>
<span class="date"> 2009.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-wang_nodes_2009')">BibTeX</button>
<button class="btn-abs" onclick="toggleTextblock('abs-wang_nodes_2009')">Abs</button>
<!-- Hidden BibTeX block -->
<div id="bib-wang_nodes_2009" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{wang_nodes_2009,
  title = {Nodes {{Placement}} for {{Optimizing Coverage}} of {{Visual Sensor Networks}}},
  booktitle = {Advances in {{Multimedia Information Processing}} ({{PCM}})},
  author = {Wang, Chang and Qi, Fei and Shi, Guang-Ming},
  editor = {Muneesawang, Paisarn and Wu, Feng and Kumazawa, Itsuo and Roeksabutr, Athikom and Liao, Mark and Tang, Xiaoou},
  year = {2009},
  pages = {1144--1149},
  publisher = {{Springer Berlin Heidelberg}},
  isbn = {978-3-642-10467-1},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}
</pre>
  </span>
</div>
<!-- Hidden abstract block -->
<div id="abs-wang_nodes_2009" style="display: none;">
  <span class="abstract hidden">
    <p>Visual sensor networks have become a research focus with its expanding application domains. How to achieve optimal coverage to improve visual network’s capability of obtaining regional information is a critical issue. As a visual sensor has a bounded field of view, a random deployment of network sensors can hardly solve this issue. This paper proposes a bounded observation field sensing model based on the sensing feature of visual sensor. According to this model, a sensor placement method is devised by means of multi-agent genetic algorithm (MAGA). The positions and poses of sensors which can enhance the coverage can be effectively worked out by this placement algorithm, thus the visual network’s capability of obtaining regional information can be improved. Experiment results show that the algorithm proposed is effective in both 2D and 3D scenes.</p>
  </span>
</div>

  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Yupin Luo, <!--and -->Dongcheng Hu.</span>
<span class="title">Distorted Grid Recognition with Its Application to Microarray Image Analysis.</span>
<span class="periodical"><em>In Third Int’l Conf. Image and Graphics,</em> 128–131, </span>
<span class="date"> Dec 2004.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_distorted_2004')">BibTeX</button>
<a href="http://doi.org/10.1109/ICIG.2004.57" target="_blank">
  <button class="btn-doi"><i class="ai ai-doi"></i> DOI</button></a>
<!-- Hidden BibTeX block -->
<div id="bib-qi_distorted_2004" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{qi_distorted_2004,
  title = {Distorted Grid Recognition with Its Application to Microarray Image Analysis},
  booktitle = {Third {{Int}}'l {{Conf}}. {{Image}} and {{Graphics}}},
  author = {Qi, Fei and Luo, Yupin and Hu, Dongcheng},
  year = {2004},
  month = dec,
  pages = {128--131},
  publisher = {{IEEE Computer Society}},
  address = {{Hongkong, China}},
  doi = {10.1109/ICIG.2004.57},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, Yupin Luo, <!--and -->Dongcheng Hu.</span>
<span class="title">Visual Tracking of Players through Occlusions in Low Resolution.</span>
<span class="periodical"><em>In Signal and Image Processing,</em></span>
<span class="date"> Aug 2004.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_visual_2004')">BibTeX</button>

<!-- Hidden BibTeX block -->
<div id="bib-qi_visual_2004" style="display: none;">
  <span class="bibtex hidden">
    <pre>@inproceedings{qi_visual_2004,
  title = {Visual {{Tracking}} of {{Players}} through {{Occlusions}} in {{Low Resolution}}},
  booktitle = {Signal and {{Image Processing}}},
  author = {Qi, Fei and Luo, Yupin and Hu, Dongcheng},
  year = {2004},
  month = aug,
  address = {{Honolulu, Hawaii, USA}},
  keywords = {publication}
}
</pre>
  </span>
</div>


  </div>
</div>
</li>
<li><div class="row">
  <div class="col">
    <span class="author">Fei Qi, <!--and -->Chengying Hua.</span>
<span class="title">Efficient Automated Microarray Image Analysis.</span>
<span class="periodical"><em>In Second In’l Conf. Image and Graphics,</em> 567–574, </span>
<span class="date"> Jul 2002.</span>
<span></span>
</div>
</div>
<div class="row">
  <div class="col">
    <button class="btn-bibtex" onclick="toggleTextblock('bib-qi_efficient_2002')">BibTeX</button>

<!-- Hidden BibTeX block -->
<div id="bib-qi_efficient_2002" style="display: none;">
  <span class="bibtex hidden">
    <pre>@incollection{qi_efficient_2002,
  title = {Efficient Automated Microarray Image Analysis},
  booktitle = {Second {{In}}'l {{Conf}}. {{Image}} and {{Graphics}}},
  author = {Qi, Fei and Hua, Chengying},
  editor = {Wei, Sui},
  year = {2002},
  month = jul,
  pages = {567--574},
  address = {{Hefei, China}},
  keywords = {publication},
  number = {4875},
  series = {Proceedings of {{SPIE}}}
}
</pre>
  </span>
</div>


  </div>
</div>
</li></ol>
</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom mt-0">
  <div class="container">
    &copy; Copyright 2014-2021 Fei Qi.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: August 19, 2021.
    
  </div>
</footer>


  </body>

</html>
