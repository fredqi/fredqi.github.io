<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Welcome to Fei Qi's homepage - 《图像处理与成像制导》作业(201503)</title>
    <link rel="stylesheet" href="../../_static/screen.css" media="screen" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
      MMLorHTML: { prefer: { Firefox: "HTML" } }
	  });
	</script>
	<link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/favmark.ico"/>
  </head>  

  <body>
    <!-- Header, including navigation bar -->
	
    <header>
	  <a href="/"><img src="https://avatars.githubusercontent.com/u/1792102?v=3" /></a>
	  <div class="site-info">
        <h1><a href="/">Fei Qi</a></h1>
        <p>Associate Professor<br> Xidian University</p>
      </div>
	</header>

    <nav>
      <a href="../../index.html" >Home</a>
      <a href="../../research/index.html" >Research</a>
      <a href="../index.html" >Teaching</a>
    </nav>

    <div id="content">
      <!-- Main contents -->
      <div id="mainContent">
        
  <div class="section" id="id1">
<h1>《图像处理与成像制导》作业(201503)<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>使用神经网络进行机器学习<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>在本项练习中，你将学会如何实现神经网络的误差反传训练算法，并应用它进行手写数字识
别。完成本项练习，你需要下载两个数据文件（ <a class="reference download internal" href="../../_downloads/HW1503data.mat"><code class="xref download docutils literal"><span class="pre">数据文件1</span></code></a> 和 <a class="reference download internal" href="../../_downloads/HW1503weights.mat"><code class="xref download docutils literal"><span class="pre">数据文件2</span></code></a> ）以及
<code class="docutils literal"><span class="pre">python</span></code> 的 <a class="reference download internal" href="../../_downloads/HW1503-neural-networks.py"><code class="xref download docutils literal"><span class="pre">源代码</span></code></a> 程序。</p>
<div class="section" id="id3">
<h3>神经网络<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id4">
<h4>数据可视化<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>本次练习所用的数据集有5000个训练样本，每个样本对应于20x20大小的灰度图像。这些训练样本包括了9-0共十个数字的手写图像。这些样本中每个像素都用浮点数表示。加载得到的数据中，每幅图像都被展开为一个400维的向量，构成了数据矩阵中的一行。完整的训练数据是一个5000x400的矩阵，其每一行为一个训练样本（数字的手写图像）。数据中，对应于数字&#8221;0&#8221;的图像被标记为&#8221;10&#8221;，而数字&#8221;1&#8221;到&#8221;9&#8221;按照其自然顺序被分别标记为&#8221;1&#8221;到&#8221;9&#8221;。</p>
<div class="math">
\[\begin{split}X = \begin{bmatrix} - &amp; (\mathbf{x}^{(1)})^T &amp; - \\
                      - &amp; (\mathbf{x}^{(2)})^T &amp; - \\
                          &amp;  \vdots &amp;  \\
                      - &amp; (\mathbf{x}^{(m)})^T &amp; - \end{bmatrix}\end{split}\]</div>
<p>程序的第一部分将加载数据并将其显示为 <a class="reference internal" href="#id5">如图1</a>
所示的形式。实现这一功能需要你补充完善函数 <code class="docutils literal"><span class="pre">display_data</span></code> 。</p>
<a class="reference internal image-reference" href="../../_images/data-array.png" id="id5"><img alt="../../_images/data-array.png" id="id5" src="../../_images/data-array.png" style="width: 50%;" /></a>
</div>
<div class="section" id="id6">
<h4>模型表示<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>我们准备训练的神经网络是一个三层的结构，一个输入层，一个隐层以及一个输出层。由于我们训练样本（图像）是20x20的，所以输入层单元数为400（不考虑额外的偏置项，如果考虑单元个数需要+1）。在我们的程序中，数据会被加载到变量
<span class="math">\(X\)</span> 和 <span class="math">\(y\)</span> 里。</p>
<p>本项练习提供了一组训练好的网络参数( <span class="math">\(\Theta^{(1)}\)</span>,
<span class="math">\(\Theta^{(2)}\)</span> ）。这些数据存储在数据文件 <code class="docutils literal"><span class="pre">HW1503weights.mat</span></code>
，在程序中被加载到变量 <code class="docutils literal"><span class="pre">Theta1</span></code> 与 <code class="docutils literal"><span class="pre">Theta2</span></code>
中。参数的维度对应于第二层有25个单元、10个输出单元（对应于10个数字的类别）的网络。</p>
</div>
<div class="section" id="id7">
<h4>前向传播与代价函数<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>现在你需要实现神经网络的代价函数及其梯度。首先需要使得函数
<code class="docutils literal"><span class="pre">nn_cost_function</span></code> 能够返回正确的代价值。</p>
<p>神经网络的代价函数（不包括正则化项）的定义为：</p>
<div class="math">
\[ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[
-y_k^{(i)} \log((h_{\theta}(x^{(i)}))_k)
-(1 - y_k^{(i)}) \log(1 - (h_{\theta}(x^{(i)}))_k)
\right]\]</div>
<p>其中 <span class="math">\(h_{\theta}(x^{(i)})\)</span>
的计算如 <a class="reference internal" href="#id8">神经网络结构图</a> 所示，<span class="math">\(K=10\)</span> 是所有可能的类别数。这里的 <span class="math">\(y\)</span> 使用了
one-hot 的表达方式。</p>
<a class="reference internal image-reference" href="../../_images/nn-representation.png" id="id8"><img alt="../../_images/nn-representation.png" id="id8" src="../../_images/nn-representation.png" style="width: 75%;" /></a>
<p>运行程序，使用预先训练好的网络参数，确认你得到的代价函数是正确的。（正确的代价约为0.287629）。</p>
</div>
<div class="section" id="id9">
<h4>代价函数的正则化<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>神经网络包括正则化项的代价函数为</p>
<div class="math">
\[\begin{split}\begin{split}
 J(\theta) &amp; = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[
-y_k^{(i)} \log((h_{\theta}(x^{(i)}))_k)
-(1 - y_k^{(i)}) \log(1 - (h_{\theta}(x^{(i)}))_k)
\right] \\
&amp; + \frac{\lambda}{m} \left[
\sum_{j=1}^{25} \sum_{k=1}^{400} (\Theta_{j,k}^{(1)})^2 +
\sum_{j=1}^{10} \sum_{k=1}^{25} (\Theta_{j,k}^{(2)})^2 \right]
\end{split}\end{split}\]</div>
<p>注意在上面式子中，正则化项的加和形式与练习中设定的网络结构一致。但是你的代码实现要保证能够用于任意大小的神经网络。</p>
<p>此外，还需要注意，对应于偏置项的参数不能包括在正则化项中。对于矩阵
<code class="docutils literal"><span class="pre">Theta1</span></code> 与 <code class="docutils literal"><span class="pre">Theta2</span></code> 而言，这些项对应于矩阵的第一列。</p>
<p>运行程序，使用预先训练好的权重数据，设置正则化系数 <span class="math">\(\lambda=1\)</span>
(<code class="docutils literal"><span class="pre">lmb</span></code>) 确认你得到的代价函数是正确的。（正确的代价约为0.383770）。</p>
</div>
</div>
<div class="section" id="id10">
<h3>误差反传训练算法<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="sigmoid">
<h4><code class="docutils literal"><span class="pre">Sigmoid</span></code> 函数及其梯度<a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h4>
<p>Sigmoid 函数定义为</p>
<div class="math">
\[\text{sigmoid}(z) = g(z) = \frac{1}{1+\exp(-z)}\]</div>
<p>Sigmoid 函数的梯度可以按照下式进行计算</p>
<div class="math">
\[g^{\prime}(z) = \frac{d}{dz} g(z) = g(z)(1-g(z))\]</div>
<p>为验证你的实现是正确的，以下事实可供你参考。当 <span class="math">\(z=0\)</span> 是，梯度的精确值为
0.25 。当 <span class="math">\(z\)</span> 的值很大（可正可负）时，梯度值接近于0。</p>
<p>这里，你需要补充完成函数 <code class="docutils literal"><span class="pre">sigmoid</span></code> 与 <code class="docutils literal"><span class="pre">sigmoid_gradient</span></code>
。你需要保证实现的函数的输入参数可以为矢量和矩阵( <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>)。</p>
</div>
<div class="section" id="id11">
<h4>网络参数的随机初始化<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p>训练神经网络时，使用随机数初始化网络参数非常重要。一个非常有效的随机初始化策略为，在范围
<span class="math">\([ -\epsilon_{init}, \epsilon_{init} ]\)</span> 内按照均匀分布随机选择参数
<span class="math">\(\Theta^{(l)}\)</span> 的初始值。这里你需要设置
<span class="math">\(\epsilon_{init} = 0.12\)</span> 。这个范围保证了参数较小且训练过程高效。</p>
<p>你需要补充实现函数 <code class="docutils literal"><span class="pre">rand_initialize_weigths</span></code> 。</p>
<p>对于一般的神经网络，如果第 <span class="math">\(l\)</span> 层的输入单元数为
<span class="math">\(L_{in}\)</span> ，输出单元数为 <span class="math">\(L_{out}\)</span> ，则
<span class="math">\(\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}\)</span>
可以做为有效的指导策略。</p>
</div>
<div class="section" id="backpropagation">
<h4>误差反传训练算法 (Backpropagation)<a class="headerlink" href="#backpropagation" title="Permalink to this headline">¶</a></h4>
<p>现在你需要实现误差反传训练算法。误差反传算法的思想大致可以描述如下。对于一个训练样本
<span class="math">\((x^{(t)}, y^{(t)})\)</span>
，我们首先使用前向传播计算网络中所有单元（神经元）的激活值（activation），包括假设输出
<span class="math">\(h_{\Theta}(x)\)</span> 。那么，对于第 <span class="math">\(l\)</span> 层的第 <span class="math">\(j\)</span>
个节点，我们期望计算出一个“误差项” <span class="math">\(\delta_{j}^{(l)}\)</span>
用于衡量该节点对于输出的误差的“贡献”。</p>
<p>对于输出节点，我们可以直接计算网络的激活值与真实目标值之间的误差。对于我们所训练的第3层为输出层的网络，这个误差定义了
<span class="math">\(\delta_{j}^{(3)}\)</span> 。对于隐层单元，需要根据第 <span class="math">\(l+1\)</span>
层的节点的误差的加权平均来计算 <span class="math">\(\delta_{j}^{(l)}\)</span> 。</p>
<a class="reference internal image-reference" href="../../_images/nn-backpropagation.png" id="id12"><img alt="../../_images/nn-backpropagation.png" id="id12" src="../../_images/nn-backpropagation.png" style="width: 75%;" /></a>
<p>下面是误差反传训练算法的细节（如 <a class="reference internal" href="#id12">图3</a> 所示）。你需要在一个循环中实现步骤1至4。循环的每一步处理一个训练样本。第5步将累积的梯度除以
<span class="math">\(m\)</span> 以得到神经网络代价函数的梯度。</p>
<ol class="arabic">
<li><p class="first">设输入层的值( <span class="math">\(a^{(1)}\)</span>)为第 <span class="math">\(t\)</span> 个训练样本
<span class="math">\(x^{(t)}\)</span>
。执行前向传播，计算第2层与第3层各节点的激活值( <span class="math">\(z^{(2)}\)</span>,
<span class="math">\(a^{(2)}\)</span>, <span class="math">\(z^{(3)}\)</span>, <span class="math">\(a^{(3)}\)</span>)。注意你需要在
<span class="math">\(a^{(1)}\)</span> 与 <span class="math">\(a^{(2)}\)</span> 增加一个全部为 +1
的向量，以确保包括了偏置项。在 <code class="docutils literal"><span class="pre">numpy</span></code> 中可以使用函数 <code class="docutils literal"><span class="pre">ones</span></code> ，
<code class="docutils literal"><span class="pre">hstack</span></code>, <code class="docutils literal"><span class="pre">vstack</span></code> 等完成（向量化版本）。</p>
</li>
<li><p class="first">对第3层中的每个输出单元 <span class="math">\(k\)</span> ，计算</p>
<div class="math">
\[\delta_{k}^{(3)} = a_{k}^{(3)} - y_k\]</div>
<p>其中 <span class="math">\(y_k \in \{0, 1\}\)</span> 表示当前训练样本是否是第 <span class="math">\(k\)</span> 类。</p>
</li>
<li><p class="first">对隐层 <span class="math">\(l=2\)</span> , 计算</p>
<div class="math">
\[\delta^{(2)} = \left( \Theta^{(2)} \right)^T \delta^{(3)} .* g^{\prime} (z^{(2)})\]</div>
<p>其中 <span class="math">\(g^{\prime}\)</span> 表示 Sigmoid 函数的梯度， <code class="docutils literal"><span class="pre">.*</span></code> 在 <code class="docutils literal"><span class="pre">numpy</span></code>
中是通常的逐个元素相乘的乘法，矩阵乘法应当使用 <code class="docutils literal"><span class="pre">numpy.dot</span></code> 函数。</p>
</li>
<li><p class="first">使用下式将当前样本梯度进行累加：</p>
<div class="math">
\[\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\]</div>
<p>在 <code class="docutils literal"><span class="pre">numpy</span></code> 中，数组可以使用 <code class="docutils literal"><span class="pre">+=</span></code> 运算。</p>
</li>
<li><p class="first">计算神经网络代价函数的（未正则化的）梯度，</p>
<div class="math">
\[\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)}\]</div>
</li>
</ol>
<p>这里，你需要（部分）完成函数 <code class="docutils literal"><span class="pre">nn_grad_function</span></code> 。程序将使用函数
<code class="docutils literal"><span class="pre">check_nn_gradients</span></code> 来检查你的实现是否正确。</p>
<p>在使用循环的方式完成函数 <code class="docutils literal"><span class="pre">nn_grad_function</span></code>
后，建议尝试使用向量化的方式重新实现这个函数。</p>
</div>
<div class="section" id="id13">
<h4>检查梯度<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<p>在神经网络中，需要最小化代价函数 <span class="math">\(J(\Theta)\)</span>
。为了检查梯度计算是否正确，考虑把参数 <span class="math">\(\Theta^{(1)}\)</span> 和
<span class="math">\(\Theta^{(2)}\)</span> 展开为一个长的向量 <span class="math">\(\theta\)</span> 。假设函数
<span class="math">\(f_i(\theta)\)</span> 表示
<span class="math">\(\frac{\partial}{\partial \theta_i} J(\theta)\)</span> 。</p>
<p>令</p>
<div class="math">
\[\begin{split}\theta^{(i+)} = \theta + \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix} \qquad \theta^{(i-)} = \theta - \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix}\end{split}\]</div>
<p>上式中， <span class="math">\(\theta^{(i+)}\)</span> 除了第 <span class="math">\(i\)</span> 个元素增加了
<span class="math">\(\epsilon\)</span> 之外，其他元素均与 <span class="math">\(\theta\)</span>
相同。类似的， <span class="math">\(\theta^{(i-)}\)</span> 中仅第 <span class="math">\(i\)</span> 个元素减少了
<span class="math">\(\epsilon\)</span> 。可以使用数值近似验证 <span class="math">\(f_i(\theta)\)</span>
计算是否正确：</p>
<div class="math">
\[f_i(\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})}{2\epsilon}\]</div>
<p>如果设 <span class="math">\(\epsilon=10^{-4}\)</span>
，通常上式左右两端的差异出现于第4位有效数字之后（经常会有更高的精度）。</p>
<p>在练习的程序代码中，函数 <code class="docutils literal"><span class="pre">compute_numerical_gradient</span></code>
已经实现，建议你认真阅读该函数并理解其实现原理与方案。</p>
<p>之后，程序将执行 <code class="docutils literal"><span class="pre">check_nn_gradients</span></code>
函数。该函数将创建一个较小的神经网络用于检测你的误差反传训练算法所计算得到的梯度是否正确。如果你的实现是正确的，你得到的梯度与数值梯度之后的绝对误差（各分量的绝对值差之和）应当小于 <span class="math">\(10^{-9}\)</span> 。</p>
</div>
<div class="section" id="id14">
<h4>神经网络的正则化<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<p>你正确实现了误差反传训练算法之后，应当在梯度中加入正则化项。</p>
<p>假设你在误差反传算法中计算了
<span class="math">\(\Delta_{ij}^{(l)}\)</span> ，你需要增加的正则化项为</p>
<div class="math">
\[\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} \qquad \text{for } j = 0\]</div>
<div class="math">
\[\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} + \frac{\lambda}{m} \Theta_{ij}^{(l)} \qquad \text{for } j \geq 1\]</div>
<p>注意你不应该正则化 <span class="math">\(\Theta^{(l)}\)</span> 的第一列，因其对应于偏置项。</p>
</div>
<div class="section" id="fmin-cg">
<h4>使用 <code class="docutils literal"><span class="pre">fmin_cg</span></code> 学习网络参数<a class="headerlink" href="#fmin-cg" title="Permalink to this headline">¶</a></h4>
<p>如果你正确实现了神经网络的代价函数与梯度计算函数，下一步就是使用
<code class="docutils literal"><span class="pre">scipy.optimize.fmin_cg</span></code> 函数学习一组较好的网络参数。</p>
<p>在训练完成后，程序会汇报在训练集上的正确率。如果你的实现正确，得到的正确率应该在
95.4% 左右（由于随机初始化的原因可能有 1% 变化）。</p>
<p>你可以调整正则化参数 <span class="math">\(\lambda\)</span> (<code class="docutils literal"><span class="pre">lmb</span></code>)
以及优化算法的最大迭代次数（如设 <code class="docutils literal"><span class="pre">maxiter</span> <span class="pre">=</span> <span class="pre">400</span></code>
），来观察各参数对训练过程和结果的影响。</p>
</div>
</div>
<div class="section" id="id15">
<h3>可视化隐层<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>理解神经网络学到什么的一种途径是将隐层单元学到的表示进行可视化。非正式的说，对一个特定的隐层单元，一种可视化其计算结果的方式是找到一个能够使其激活（即其activation
value (<span class="math">\(a_{i}^{(l)}\)</span> 接近于1）输入 <span class="math">\(\mathbf{x}\)</span> 。</p>
<p>对于我们学得的神经网络，一种可视化其隐层所学得的“表示”的方式是将除偏置单元外的
400 维向量转换为 20x20 的图像并显示出来。</p>
</div>
</div>
</div>


      </div>
    </div>

	<!-- Footer -->

    <footer>
        <div class="copyright"><ul>
          <li>Last updated on Jan 20, 2016.</li>
          <li>&copy; Copyright 2009-2016, Fei Qi.</li>
        </ul></div>

		<div class="socialnet">
<a href="mailto:fred.qi@ieee.org">
<svg width="40px" height="40px" viewBox="0 0 60 60" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <path d="M0.224580688,30 C0.224580688,13.4314567 13.454941,0 29.7754193,0 C46.0958976,0 59.3262579,13.4314567 59.3262579,30 C59.3262579,46.5685433 46.0958976,60 29.7754193,60 C13.454941,60 0.224580688,46.5685433 0.224580688,30 Z M0.224580688,30" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
    <path d="M35.0384324,31.6384006 L47.2131148,40.5764264 L47.2131148,20 L35.0384324,31.6384006 Z M13.7704918,20 L13.7704918,40.5764264 L25.9449129,31.6371491 L13.7704918,20 Z M30.4918033,35.9844891 L27.5851037,33.2065217 L13.7704918,42 L47.2131148,42 L33.3981762,33.2065217 L30.4918033,35.9844891 Z M46.2098361,20 L14.7737705,20 L30.4918033,32.4549304 L46.2098361,20 Z M46.2098361,20" id="Shape" fill="#333333" sketch:type="MSShapeGroup"></path>
    <path d="M59.3262579,30 C59.3262579,46.5685433 46.0958976,60 29.7754193,60 C23.7225405,60 18.0947051,58.1525134 13.4093244,54.9827754 L47.2695458,5.81941103 C54.5814438,11.2806503 59.3262579,20.0777973 59.3262579,30 Z M59.3262579,30" id="reflec" fill-opacity="0.08" fill="#000000" sketch:type="MSShapeGroup"></path>
</svg>
</a>
<a href="http://github.com/fredqi/fredqi.github.io">
<svg width="40px" height="40px" viewBox="0 0 60 60" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <path d="M0.336871032,30 C0.336871032,13.4314567 13.5672313,0 29.8877097,0 C46.208188,0 59.4385483,13.4314567 59.4385483,30 C59.4385483,46.5685433 46.208188,60 29.8877097,60 C13.5672313,60 0.336871032,46.5685433 0.336871032,30 Z M0.336871032,30" id="Github" fill="#333333" sketch:type="MSShapeGroup"></path>
    <path d="M18.2184245,31.9355566 C19.6068506,34.4507902 22.2845295,36.0156764 26.8007287,36.4485173 C26.1561023,36.9365335 25.3817877,37.8630984 25.2749857,38.9342607 C24.4644348,39.4574749 22.8347506,39.62966 21.5674303,39.2310659 C19.7918469,38.6717023 19.1119377,35.1642642 16.4533306,35.6636959 C15.8773626,35.772144 15.9917933,36.1507609 16.489567,36.4722998 C17.3001179,36.9955141 18.0629894,37.6500075 18.6513541,39.04366 C19.1033554,40.113871 20.0531304,42.0259813 23.0569369,42.0259813 C24.2489236,42.0259813 25.0842679,41.8832865 25.0842679,41.8832865 C25.0842679,41.8832865 25.107154,44.6144649 25.107154,45.6761142 C25.107154,46.9004355 23.4507693,47.2457569 23.4507693,47.8346108 C23.4507693,48.067679 23.9990832,48.0895588 24.4396415,48.0895588 C25.3102685,48.0895588 27.1220883,47.3646693 27.1220883,46.0918317 C27.1220883,45.0806012 27.1382993,41.6806599 27.1382993,41.0860982 C27.1382993,39.785673 27.8372803,39.3737607 27.8372803,39.3737607 C27.8372803,39.3737607 27.924057,46.3153869 27.6704022,47.2457569 C27.3728823,48.3397504 26.8360115,48.1846887 26.8360115,48.6727049 C26.8360115,49.3985458 29.0168704,48.8505978 29.7396911,47.2571725 C30.2984945,46.0166791 30.0543756,39.2072834 30.0543756,39.2072834 L30.650369,39.1949165 C30.650369,39.1949165 30.6837446,42.3123222 30.6637192,43.7373675 C30.6427402,45.2128317 30.5426134,47.0792797 31.4208692,47.9592309 C31.9977907,48.5376205 33.868733,49.5526562 33.868733,48.62514 C33.868733,48.0857536 32.8436245,47.6424485 32.8436245,46.1831564 L32.8436245,39.4688905 C33.6618042,39.4688905 33.5387911,41.6768547 33.5387911,41.6768547 L33.5988673,45.7788544 C33.5988673,45.7788544 33.4186389,47.2733446 35.2190156,47.8992991 C35.8541061,48.1209517 37.2139245,48.1808835 37.277815,47.8089257 C37.3417055,47.4360167 35.6405021,46.8814096 35.6252446,45.7236791 C35.6157088,45.0178155 35.6567131,44.6059032 35.6567131,41.5379651 C35.6567131,38.470027 35.2438089,37.336079 33.8048426,36.4323453 C38.2457082,35.9766732 40.9939527,34.880682 42.3337458,31.9450695 C42.4383619,31.9484966 42.8791491,30.5737742 42.8219835,30.5742482 C43.1223642,29.4659853 43.2844744,28.1550957 43.3168964,26.6025764 C43.3092677,22.3930799 41.2895654,20.9042975 40.9014546,20.205093 C41.4736082,17.0182425 40.8060956,15.5675121 40.4961791,15.0699829 C39.3518719,14.6637784 36.5149435,16.1145088 34.9653608,17.1371548 C32.438349,16.3998984 27.0982486,16.4712458 25.0957109,17.3274146 C21.4005522,14.6875608 19.445694,15.0918628 19.445694,15.0918628 C19.445694,15.0918628 18.1821881,17.351197 19.1119377,20.6569598 C17.8961113,22.2028201 16.9902014,23.2968136 16.9902014,26.1963718 C16.9902014,27.8297516 17.1828264,29.2918976 17.6176632,30.5685404 C17.5643577,30.5684093 18.2008493,31.9359777 18.2184245,31.9355566 Z M18.2184245,31.9355566" id="Path" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
    <path d="M59.4385483,30 C59.4385483,46.5685433 46.208188,60 29.8877097,60 C23.8348308,60 18.2069954,58.1525134 13.5216148,54.9827754 L47.3818361,5.81941103 C54.6937341,11.2806503 59.4385483,20.0777973 59.4385483,30 Z M59.4385483,30" id="reflec" fill-opacity="0.08" fill="#000000" sketch:type="MSShapeGroup"></path>
</svg>
</a>
<a href="http://linkedin.com/pub/fei-qi/36/9a1/39a/">
<svg width="40px" height="40px" viewBox="0 0 60 60" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <path d="M0.449161376,30 C0.449161376,13.4314567 13.6795217,0 30,0 C46.3204783,0 59.5508386,13.4314567 59.5508386,30 C59.5508386,46.5685433 46.3204783,60 30,60 C13.6795217,60 0.449161376,46.5685433 0.449161376,30 Z M0.449161376,30" fill="#007BB6" sketch:type="MSShapeGroup"></path>
    <path d="M22.4680392,23.7098144 L15.7808366,23.7098144 L15.7808366,44.1369537 L22.4680392,44.1369537 L22.4680392,23.7098144 Z M22.4680392,23.7098144" id="Path" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
    <path d="M22.9084753,17.3908761 C22.8650727,15.3880081 21.4562917,13.862504 19.1686418,13.862504 C16.8809918,13.862504 15.3854057,15.3880081 15.3854057,17.3908761 C15.3854057,19.3522579 16.836788,20.9216886 19.0818366,20.9216886 L19.1245714,20.9216886 C21.4562917,20.9216886 22.9084753,19.3522579 22.9084753,17.3908761 Z M22.9084753,17.3908761" id="Path" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
    <path d="M46.5846502,32.4246563 C46.5846502,26.1503226 43.2856534,23.2301456 38.8851658,23.2301456 C35.3347011,23.2301456 33.7450983,25.2128128 32.8575489,26.6036896 L32.8575489,23.7103567 L26.1695449,23.7103567 C26.2576856,25.6271338 26.1695449,44.137496 26.1695449,44.137496 L32.8575489,44.137496 L32.8575489,32.7292961 C32.8575489,32.1187963 32.9009514,31.5097877 33.0777669,31.0726898 C33.5610713,29.8530458 34.6614937,28.5902885 36.5089747,28.5902885 C38.9297703,28.5902885 39.8974476,30.4634101 39.8974476,33.2084226 L39.8974476,44.1369537 L46.5843832,44.1369537 L46.5846502,32.4246563 Z M46.5846502,32.4246563" id="Path" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
    <path d="M59.5508386,30 C59.5508386,46.5685433 46.3204783,60 30,60 C23.9471212,60 18.3192858,58.1525134 13.6339051,54.9827754 L47.4941264,5.81941103 C54.8060245,11.2806503 59.5508386,20.0777973 59.5508386,30 Z M59.5508386,30" id="reflec" fill-opacity="0.08" fill="#000000" sketch:type="MSShapeGroup"></path>
</svg>
</a>
<a href="http://facebook.com/fredqi.vision">
<svg width="40px" height="40px" viewBox="0 0 60 60" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:sketch="http://www.bohemiancoding.com/sketch/ns">
    <path d="M0.112290344,30 C0.112290344,13.4314567 13.3426506,0 29.663129,0 C45.9836073,0 59.2139676,13.4314567 59.2139676,30 C59.2139676,46.5685433 45.9836073,60 29.663129,60 C13.3426506,60 0.112290344,46.5685433 0.112290344,30 Z M0.112290344,30" fill="#3B5998" sketch:type="MSShapeGroup"></path>
	<path d="M32.1341457,46.3196729 L32.1341457,29.9980891 L36.5657565,29.9980891 L37.1530406,24.3735809 L32.1341457,24.3735809 L32.141675,21.5584604 C32.141675,20.091502 32.2787707,19.3054722 34.351206,19.3054722 L37.1216686,19.3054722 L37.1216686,13.6803271 L32.6894304,13.6803271 C27.3655995,13.6803271 25.491749,16.4088187 25.491749,20.9972835 L25.491749,24.3742179 L22.1732173,24.3742179 L22.1732173,29.998726 L25.491749,29.998726 L25.491749,46.3196729 L32.1341457,46.3196729 Z M32.1341457,46.3196729" id="Path" fill="#FFFFFF" sketch:type="MSShapeGroup"></path>
	<path d="M59.2139676,30 C59.2139676,46.5685433 45.9836073,60 29.663129,60 C23.6102502,60 17.9824147,58.1525134 13.2970341,54.9827754 L47.1572554,5.81941103 C54.4691534,11.2806503 59.2139676,20.0777973 59.2139676,30 Z M59.2139676,30" id="reflec" fill-opacity="0.08" fill="#000000" sketch:type="MSShapeGroup"></path>
</svg>
</a>
       </div>
    </footer></body>
</html>  